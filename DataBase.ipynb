{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from typing import Union, Optional, Tuple\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import choices\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from transformers import WhisperModel, WhisperFeatureExtractor\n",
    "from transformers import WhisperProcessor, WhisperModel\n",
    "import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPreprocessor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        target_sr: int = 16000, \n",
    "        normalize: bool = True,\n",
    "        trim_silence: bool = True,\n",
    "        max_duration: Optional[float] = None,\n",
    "        mono: bool = True\n",
    "    ):\n",
    "        self.target_sr = target_sr\n",
    "        self.normalize = normalize\n",
    "        self.trim_silence = trim_silence\n",
    "        self.max_duration = max_duration\n",
    "        self.mono = mono\n",
    "        \n",
    "        \n",
    "    def process(self, audio_path: Union[str, np.ndarray]) -> np.ndarray:\n",
    "        if isinstance(audio_path, str):\n",
    "            audio, orig_sr = librosa.load(audio_path, sr=None, mono=False)\n",
    "            if self.mono and audio.ndim > 1:\n",
    "                audio = librosa.to_mono(audio)\n",
    "        elif isinstance(audio_path, np.ndarray):\n",
    "            audio, orig_sr = audio_path, self.target_sr\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a file path or numpy array\")\n",
    "\n",
    "        if orig_sr != self.target_sr:\n",
    "            audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=self.target_sr)\n",
    "            \n",
    "        peak_normalization = audio / np.max(np.abs(audio))\n",
    "        rms_normalization = audio / np.sqrt(np.mean(audio**2))\n",
    "        audio = peak_normalization * 0.5 + rms_normalization * 0.5\n",
    "        \n",
    "        if self.trim_silence:\n",
    "            audio, _= librosa.effects.trim(audio)\n",
    "        if self.max_duration is not None:\n",
    "            max_length = int(self.max_duration * self.target_sr)\n",
    "            audio = audio[:max_length]\n",
    "        audio = np.array(audio, dtype=np.float16)\n",
    "        \n",
    "        if len(audio) < self.target_sr:\n",
    "            pad_length = self.target_sr - len(audio)\n",
    "            audio = np.pad(audio, (0, pad_length), mode='constant')\n",
    "        else:\n",
    "            audio = audio[:self.target_sr]\n",
    "            \n",
    "        return audio\n",
    "    \n",
    "    \n",
    "\n",
    "    def save_processed_audio(\n",
    "        self, \n",
    "        audio: np.ndarray, \n",
    "        output_path: str, \n",
    "        format: str = 'wav'\n",
    "    ):\n",
    "        sf.write(output_path, audio, self.target_sr, format=format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing AudioDataset ...\n",
      "Current Language: ar\n",
      "Current Language: as\n",
      "Current Language: cs\n",
      "Current Language: dv\n",
      "Current Language: fa\n",
      "Current Language: fr\n",
      "Current Language: ru\n",
      "Current Language: ta\n",
      "Current Language: tr\n",
      "Current Language: zh-CN\n",
      "KeyWord Data Processed ...\n",
      "Query Data Processed ...\n"
     ]
    }
   ],
   "source": [
    "class AudioFolderProcessor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.preprocessor = AudioPreprocessor()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    \n",
    "    def process_audio_dataset_train(self, base_dir='/Users/anshsingh200516/Desktop/GUI/TRAIN'):\n",
    "        \n",
    "        audio_data = []\n",
    "        \n",
    "        if not os.path.exists(base_dir):\n",
    "            raise FileNotFoundError(f\"Base directory {base_dir} not found\")\n",
    "        \n",
    "        for language in os.listdir(base_dir):\n",
    "            print(f\"Current Language: {language}\")\n",
    "            language_path = os.path.join(base_dir, language)\n",
    "            \n",
    "            if not os.path.isdir(language_path):\n",
    "                continue\n",
    "            \n",
    "            for keyword in os.listdir(language_path):\n",
    "                keyword_path = os.path.join(language_path, keyword)\n",
    "                \n",
    "                if not os.path.isdir(keyword_path):\n",
    "                    continue\n",
    "                for audio_file in os.listdir(keyword_path):\n",
    "                    if audio_file.lower().endswith(('.wav', '.opus')):\n",
    "                        audio_path = os.path.join(keyword_path, audio_file)\n",
    "                        audio_array = self.preprocessor.process(audio_path)\n",
    "                        audio_data.append({\n",
    "                            'language': language,\n",
    "                            'keyword': keyword,\n",
    "                            'audio_array': audio_array\n",
    "                        })\n",
    "        \n",
    "\n",
    "        df = pd.DataFrame(audio_data)\n",
    "        return df, 0\n",
    "    \n",
    "    def process_audio_dataset_query(self, base_dir='TEST_DUMMY'):\n",
    "        audio_data = []\n",
    "        \n",
    "        if not os.path.exists(base_dir):\n",
    "            raise FileNotFoundError(f\"Base directory {base_dir} not found\")\n",
    "        \n",
    "        for audio_file in os.listdir(base_dir):\n",
    "            if audio_file.lower().endswith(('.wav', '.opus')):\n",
    "                audio_path = os.path.join(base_dir, audio_file)\n",
    "                audio_array = self.preprocessor.process(audio_path)\n",
    "                audio_data.append({\n",
    "                    'filename': audio_file,\n",
    "                    'filepath': audio_path,\n",
    "                    'file_extension': os.path.splitext(audio_file)[1],\n",
    "                    'file_size': os.path.getsize(audio_path),\n",
    "                    'audio_array': audio_array,\n",
    "                    'audio_duration': librosa.get_duration(y=audio_array, sr=16000),\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(audio_data)\n",
    "        stats = {\n",
    "            'total_audio_files': len(df),\n",
    "            'total_duration_hours': df['audio_duration'].sum() / 3600,\n",
    "            'average_duration_seconds': df['audio_duration'].mean(),\n",
    "            'min_duration_seconds': df['audio_duration'].min(),\n",
    "            'max_duration_seconds': df['audio_duration'].max(),\n",
    "        }\n",
    "        \n",
    "        return df, stats\n",
    "    \n",
    "    def process_audio_dataset(self, base_dir='/Users/anshsingh200516/Desktop/GUI/TRAIN', query=False):\n",
    "        if not query:\n",
    "            return self.process_audio_dataset_train(base_dir)\n",
    "        \n",
    "        if query:\n",
    "            return self.process_audio_dataset_query(base_dir)\n",
    "    \n",
    "    \n",
    "class AudioDataset():\n",
    "    def __init__(self, TrainDir = 'TRAIN/TRAIN', QueryDir = 'TEST_DUMMY_CORRECTED/TEST_DUMMY_CORRECTED', KW_TO_ID = r'kw_to_id.pkl'):\n",
    "        print('Initializing AudioDataset ...')\n",
    "        self.audio_processor = AudioFolderProcessor()\n",
    "        self.KeywordDir = TrainDir\n",
    "        self.QueryDir = QueryDir\n",
    "        self.KeyWordData, self.KeywordStats = self.audio_processor.process_audio_dataset(base_dir=self.KeywordDir, query=False)\n",
    "        print('KeyWord Data Processed ...')\n",
    "        try:\n",
    "            self.QueryData, self.QueryStats = self.audio_processor.process_audio_dataset(base_dir=self.QueryDir, query=True)\n",
    "            print('Query Data Processed ...')\n",
    "        except KeyError:\n",
    "            print('Query Data Not Found ...')\n",
    "        self.KW_TO_ID = pickle.load(open(KW_TO_ID, 'rb'))\n",
    "        self.ID_TO_KW = {v: k for k, v in self.KW_TO_ID.items()}\n",
    "        \n",
    "        \n",
    "    class KeywordDataset(Dataset):\n",
    "        def __init__(self, data, KW_TO_ID):\n",
    "            super().__init__()\n",
    "            self.data = data\n",
    "            self.KW_TO_ID = KW_TO_ID\n",
    "            \n",
    "\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "                \n",
    "                \n",
    "            array = self.data['audio_array'][idx]\n",
    "            id = int((self.KW_TO_ID[self.data['keyword'][idx]]))\n",
    "            one_hot_id = torch.nn.functional.one_hot(torch.tensor(id), num_classes=441).float()\n",
    "            return (array, one_hot_id)\n",
    "        \n",
    "    class QueryDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            super().__init__()\n",
    "            self.data = data\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return None\n",
    "        \n",
    "    def get_keyword_dataset(self):\n",
    "        return (self.KeywordDataset(self.KeyWordData, self.KW_TO_ID))\n",
    "    \n",
    "    def get_query_dataset(self):\n",
    "        return (self.QueryDataset(self.QueryData, self.KW_TO_ID))\n",
    "    \n",
    "    def get_keyword_dataloader(self):\n",
    "        \n",
    "        def train_test_val_split(dataset, test_size=0.2, val_size=0.1):\n",
    "            train_idx, test_idx = train_test_split(range(len(dataset)), test_size=test_size)\n",
    "            train_idx, val_idx = train_test_split(train_idx, test_size=val_size / (1 - test_size))\n",
    "            return train_idx, val_idx, test_idx\n",
    "\n",
    "        train_idx, val_idx, test_idx = train_test_val_split(self.KeywordDataset(self.KeyWordData, self.KW_TO_ID))\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "        train_loader = DataLoader(self.KeywordDataset(self.KeyWordData, self.KW_TO_ID), batch_size=32, sampler=train_sampler)\n",
    "        val_loader = DataLoader(self.KeywordDataset(self.KeyWordData, self.KW_TO_ID), batch_size=32, sampler=val_sampler)\n",
    "        test_loader = DataLoader(self.KeywordDataset(self.KeyWordData, self.KW_TO_ID), batch_size=32, sampler=test_sampler)\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "    def get_query_dataloader(self):\n",
    "        DS = self.QueryDataset(self.QueryData)\n",
    "        return DataLoader(DS, batch_size=32, shuffle=True)\n",
    "    \n",
    "ds = AudioDataset()\n",
    "KWDS = ds.get_keyword_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = ds.get_keyword_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPreprocessor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        target_sr: int = 16000, \n",
    "        normalize: bool = True,\n",
    "        trim_silence: bool = True,\n",
    "        max_duration: Optional[float] = None,\n",
    "        mono: bool = True\n",
    "    ):\n",
    "        self.target_sr = target_sr\n",
    "        self.normalize = normalize\n",
    "        self.trim_silence = trim_silence\n",
    "        self.max_duration = max_duration\n",
    "        self.mono = mono\n",
    "        \n",
    "        \n",
    "    def process(self, audio_path: Union[str, np.ndarray]) -> np.ndarray:\n",
    "        if isinstance(audio_path, str):\n",
    "            audio, orig_sr = librosa.load(audio_path, sr=None, mono=False)\n",
    "            if self.mono and audio.ndim > 1:\n",
    "                audio = librosa.to_mono(audio)\n",
    "        elif isinstance(audio_path, np.ndarray):\n",
    "            audio, orig_sr = audio_path, self.target_sr\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a file path or numpy array\")\n",
    "\n",
    "        if orig_sr != self.target_sr:\n",
    "            audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=self.target_sr)\n",
    "            \n",
    "        peak_normalization = audio / np.max(np.abs(audio))\n",
    "        rms_normalization = audio / np.sqrt(np.mean(audio**2))\n",
    "        audio = peak_normalization * 0.5 + rms_normalization * 0.5\n",
    "        \n",
    "        if self.trim_silence:\n",
    "            audio, _= librosa.effects.trim(audio)\n",
    "        if self.max_duration is not None:\n",
    "            max_length = int(self.max_duration * self.target_sr)\n",
    "            audio = audio[:max_length]\n",
    "        audio = np.array(audio, dtype=np.float16)\n",
    "        \n",
    "        if len(audio) < self.target_sr:\n",
    "            pad_length = self.target_sr - len(audio)\n",
    "            audio = np.pad(audio, (0, pad_length), mode='constant')\n",
    "        else:\n",
    "            audio = audio[:self.target_sr]\n",
    "            \n",
    "        return audio\n",
    "    \n",
    "    \n",
    "\n",
    "    def save_processed_audio(\n",
    "        self, \n",
    "        audio: np.ndarray, \n",
    "        output_path: str, \n",
    "        format: str = 'wav'\n",
    "    ):\n",
    "        sf.write(output_path, audio, self.target_sr, format=format)\n",
    "\n",
    "\n",
    "class KeywordDataset(Dataset):\n",
    "        def __init__(self, data, KW_TO_ID):\n",
    "            super().__init__()\n",
    "            self.data = data\n",
    "            self.KW_TO_ID = KW_TO_ID\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            array = self.data['audio_array'][idx]\n",
    "            id = int((self.KW_TO_ID[self.data['keyword'][idx]]))\n",
    "            one_hot_id = torch.nn.functional.one_hot(torch.tensor(id), num_classes=441).float()\n",
    "            return (array, one_hot_id)\n",
    "        \n",
    "KW_TO_ID = pickle.load(open(r'kw_to_id.pkl', 'rb'))\n",
    "KeyWordData = ds.KeyWordData\n",
    "def train_test_val_split(dataset, test_size=0.2, val_size=0.1):\n",
    "    train_idx, test_idx = train_test_split(range(len(dataset)), test_size=test_size)\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=val_size / (1 - test_size))\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "train_idx, val_idx, test_idx = train_test_val_split(KeywordDataset(KeyWordData, KW_TO_ID))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "train_loader = DataLoader(KeywordDataset(KeyWordData, KW_TO_ID), batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(KeywordDataset(KeyWordData, KW_TO_ID), batch_size=32, sampler=val_sampler)\n",
    "test_loader = DataLoader(KeywordDataset(KeyWordData, KW_TO_ID), batch_size=32, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 16000]), torch.Size([32, 441]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0].shape, sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 2.6795, Val Loss 1.9553, Val Accuracy 68.37%\n",
      "Epoch 2: Train Loss 2.0712, Val Loss 1.6713, Val Accuracy 73.81%\n",
      "Epoch 3: Train Loss 1.7700, Val Loss 1.3301, Val Accuracy 78.04%\n",
      "Epoch 4: Train Loss 1.6080, Val Loss 1.2417, Val Accuracy 79.00%\n",
      "Epoch 5: Train Loss 1.5050, Val Loss 1.0812, Val Accuracy 81.57%\n",
      "Epoch 6: Train Loss 1.4270, Val Loss 1.0712, Val Accuracy 81.11%\n",
      "Epoch 7: Train Loss 1.3372, Val Loss 0.9345, Val Accuracy 82.98%\n",
      "Epoch 8: Train Loss 1.2765, Val Loss 0.9137, Val Accuracy 85.06%\n",
      "Epoch 9: Train Loss 1.2366, Val Loss 0.9569, Val Accuracy 83.15%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, input_size=25600, num_classes=441):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, _, embeddings, __):\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=50):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "    modelWhisper = WhisperModel.from_pretrained(\"openai/whisper-base\").encoder.to(device)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for array, labels in train_loader:\n",
    "            with torch.no_grad():\n",
    "                inputs = processor(array.cpu().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "                inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "                encoder_outputs = modelWhisper(inputs['input_features'])\n",
    "                embeddings = encoder_outputs.last_hidden_state.cpu().numpy()[:,:50,:]\n",
    "                embeddings = torch.tensor(embeddings).to(device)\n",
    "                embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(None, embeddings, None)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for array, labels in val_loader:\n",
    "                inputs = processor(array.cpu().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "                inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "                encoder_outputs = modelWhisper(inputs['input_features'])\n",
    "                embeddings = encoder_outputs.last_hidden_state.cpu().numpy()[:,:50,:]\n",
    "                embeddings = torch.tensor(embeddings).to(device)\n",
    "                embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "            \n",
    "                labels = labels.to(device)\n",
    "                outputs = model(None, embeddings, None)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, actual = torch.max(labels, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == actual).sum().item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss {train_loss/len(train_loader):.4f}, '\n",
    "              f'Val Loss {val_loss/len(val_loader):.4f}, '\n",
    "              f'Val Accuracy {100 * correct/total:.2f}%')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = EmbeddingClassifier()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.to(device).eval()\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "    modelWhisper = WhisperModel.from_pretrained(\"openai/whisper-base\").encoder.to(device)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            for array, labels in val_loader:\n",
    "                inputs = processor(array.cpu().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "                inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "                encoder_outputs = modelWhisper(inputs['input_features'])\n",
    "                embeddings = encoder_outputs.last_hidden_state.cpu().numpy()[:,:50,:]\n",
    "                embeddings = torch.tensor(embeddings).to(device)\n",
    "                embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "                labels = labels.to(device)\n",
    "            outputs = model(None, embeddings, None)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, actual = torch.max(labels, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == actual).sum().item()\n",
    "    \n",
    "    print(f'Test Accuracy: {100 * correct/total:.2f}%')\n",
    "    \n",
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kws/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/f4/zq4dk9gx23x4tx9wtn9dfgv00000gn/T/ipykernel_2904/2615353314.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_cpu.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from typing import Union, Optional, Tuple\n",
    "import torch\n",
    "from transformers import WhisperModel, WhisperFeatureExtractor\n",
    "from transformers import WhisperProcessor, WhisperModel\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        target_sr: int = 16000, \n",
    "        normalize: bool = True,\n",
    "        trim_silence: bool = True,\n",
    "        max_duration: Optional[float] = None,\n",
    "        mono: bool = True\n",
    "    ):\n",
    "        self.target_sr = target_sr\n",
    "        self.normalize = normalize\n",
    "        self.trim_silence = trim_silence\n",
    "        self.max_duration = max_duration\n",
    "        self.mono = mono\n",
    "        \n",
    "        \n",
    "    def process(self, audio_path: Union[str, np.ndarray]) -> np.ndarray:\n",
    "        if isinstance(audio_path, str):\n",
    "            audio, orig_sr = librosa.load(audio_path, sr=None, mono=False)\n",
    "            if self.mono and audio.ndim > 1:\n",
    "                audio = librosa.to_mono(audio)\n",
    "        elif isinstance(audio_path, np.ndarray):\n",
    "            audio, orig_sr = audio_path, self.target_sr\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a file path or numpy array\")\n",
    "\n",
    "        if orig_sr != self.target_sr:\n",
    "            audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=self.target_sr)\n",
    "            \n",
    "        peak_normalization = audio / np.max(np.abs(audio))\n",
    "        rms_normalization = audio / np.sqrt(np.mean(audio**2))\n",
    "        audio = peak_normalization * 0.5 + rms_normalization * 0.5\n",
    "        \n",
    "        if self.trim_silence:\n",
    "            audio, _= librosa.effects.trim(audio)\n",
    "        if self.max_duration is not None:\n",
    "            max_length = int(self.max_duration * self.target_sr)\n",
    "            audio = audio[:max_length]\n",
    "        audio = np.array(audio, dtype=np.float16)\n",
    "        \n",
    "        if len(audio) < self.target_sr:\n",
    "            pad_length = self.target_sr - len(audio)\n",
    "            audio = np.pad(audio, (0, pad_length), mode='constant')\n",
    "        else:\n",
    "            audio = audio[:self.target_sr]\n",
    "            \n",
    "        return audio\n",
    "    \n",
    "\n",
    "    def save_processed_audio(\n",
    "        self, \n",
    "        audio: np.ndarray, \n",
    "        output_path: str, \n",
    "        format: str = 'wav'\n",
    "    ):\n",
    "        sf.write(output_path, audio, self.target_sr, format=format)\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, input_size=25600, num_classes=441):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, _, embeddings, __):\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "def inference(model, audio_path, device):\n",
    "    model.load_state_dict(torch.load('best_model_cpu.pth'))\n",
    "    model.to(device).eval()\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "    modelWhisper = WhisperModel.from_pretrained(\"openai/whisper-base\").encoder.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        audio_processor = AudioPreprocessor()\n",
    "        audio_array = audio_processor.process(audio_path)\n",
    "        inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        encoder_outputs = modelWhisper(inputs['input_features'])\n",
    "        embeddings = encoder_outputs.last_hidden_state.cpu().numpy()[:,:50,:]\n",
    "        embeddings = torch.tensor(embeddings).to(device)\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "        outputs = model(None, embeddings, None)\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "device = 'cpu'\n",
    "model = EmbeddingClassifier()\n",
    "audio_path = 'TEST_DUMMY_FINAL/1.wav'\n",
    "inference(model, audio_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
